\section{Discussion}

\todo{Suggest try and strucuture this a bit more e.g. each para bold the key lesson @ start of para. Can you group into implications for research/implications for practice to make more actionable??}

This paper presents the interactive type debugging tool \chameleon{} and charts the evolution of its design across several iterations in response to user evaluation and feedback, as well as examining the effectiveness of the general approach compared to traditional static type error messages. We found that programmers using \chameleon{} type compare tool are able to debug errors faster than using traditional text-based error messages. This effect is shown more clearly when the task is not trivial. We found that programmers who actively use ChameleonIDE's interactive features (candidate expression cards and deduction steps) are more efficient in fixing type errors than simply reading the type error output. In this section, we will discuss a few interpretations of the results.


From the results of user studies 1 and 2, we observed that the choice of debugging tool had little effect on how fast programmers solve type errors. Conversely, when facing more realistic problems (longer source code, error locations more scattered), programmers are more effective using \chameleon{}. We believe one factor of this effectiveness of \chameleon{} is to reduce the amount of reading time. Jbara \cite{jbara_how_2015} showed that reading source code is generally the initial step of solving programming problems and is done in several passes. Although traditional compiler error message tools initially show fewer locations, it is unreliable, meaning that programmers have to expand the reading span without clear guidance. In contrast, Chameleon shows more error locations initially. However, the completeness of error locations assures programmers which part of the source code can be safely skipped.


From the results of user studies 3 and 4, we found that programmers who use interactive tool fix type errors faster than the ones who passively read the error output. This difference is stronger in harder tasks. We speculate that one factor of this result is that  \chameleon{} helps to develop debugging plans. We observed that when working with \chameleon{}, programmers form different debugging tactics to attack the problem. Among the high interactivity participants in user study 4, some programmers cycle through deduction steps as a guide to reading source code; some programmers navigate to both ends of the deduction steps where types are normally grounded and concrete. In contrast, minimal interactive participants generally form more similar plans, including carefully reading the program text and manually annotating expressions based on their understanding of the program.

\begin{listing}[!ht]
\begin{minted}{haskell}
f z     
    | z == 3 = False
    | z == '4' = True
\end{minted}
\caption{     In this simple program, \chameleon{} report the error happened in \texttt{f} and \texttt{z}.}
\label{listing:2}
\end{listing}

We speculate another factor of the effectiveness \chameleon{} interactive debugging tools is they help programmers effectively chunk intermediate information. With the program showing in the listing \ref{listing:2}, \chameleon{} offers two candidate expressions: \texttt{f} can be typed as \texttt{Int -> Bool} or \texttt{Char -> Bool}; \texttt{z} can be typed as \texttt{Int} or \texttt{Char}. Although, in theory, these two statements are to the same effect, programmers are often required to compute the latter from the former or vice versa. And this computation may carry out multiple layers. Programmers have to remember all the intermediate types and their reasoning throughout such mental gymnastics. Assisted by candidate expression cards and deduction steps, this intermediate information is externalized on screen and can be retrieved anytime. A recent study on working memory \cite{crichton_role_2021} suggested this approach may provide a positive effect in helping programmers manage cognitive load and free up working-memory space for high-level thinking.


In our studies, \chameleon{} shows a few different designs for type error visualization and interaction. However, we are far from exhausting the search space. One challenge we notice in the current design is that programmers have to shift their focus between the editor on the left and the \chameleon{} debugging interface on the right. Using the hover popup window in most mainstream IDEs may reduce the context switching during debugging.

On a similar note, the current implementation of \chameleon{} requires non-trivial adaptation for editors such as VS Code and IntelliJ due to the overlay explanation layer. This type of error visualization is non-standard in mainstream editors. However, there may exist alternative representations of \chameleon{} error reporting using only the features available in major editors and IDEs. It is especially beneficial to represent \chameleon{} errors using a universal debugging middle layer such as language server protocol (LSP). This will allow \chameleon{}  to be adapted into various coding environments which support an LSP back end.

Another area for future study is to extend the evaluation process. In studies 1b and 2, we used harder tasks compared to study 1a. However, the difficulty level is nowhere as hard as a professional Haskell programmer may face in the production Haskell codebase. It would be interesting to see how \chameleon{}  faces type errors that span multiple files and packages. Another dimension of difficulty is higher-level extractions. Haskell programs get more tricky to the debugger when more abstractions are added to the codebase, such as Monads, Monad transformers, and Lenses. These topics were not included in our study to accommodate novice users. However, observing how expert Haskell programmers use \chameleon{} features to solve advanced problems will gain valuable insight into the true usability of the tool.

Another direction to expand the work is in data collection. The online study limited the amount of qualitative data we could gather from the users. Although we handed out debriefing surveys after each study, it is no substitute for more in-depth qualitative study methods. Inviting users for speak-aloud walkthroughs and interviews are viable methods to unveil deeper usability issues about \chameleon{}.

Our study focuses on the Haskell language for its popularity in the academic world. However, the low quality of textual type errors is a problem not limited to Haskell. Modern statically typed languages more or less all share the same problem. We believe the underlying type reporting technique and algorithms can be generalized to other languages. It will be exciting to see how interactive debugging features perform in other paradigms, such as  imperative languages and incremental type systems.

Our design of \chameleon{}  emphasizes allowing programmers to update the type error based on their prior knowledge. It unfolds to answer three questions: which expression should be considered un-typeable? Between multiple possible types, which one should be considered intended? Among all the possible locations, which one should be considered the cause? \chameleon{} answers all three questions by leaving it to the programmers to decide. However, it is possible and likely that some programmers enjoy conclusive error messages instead of flexible ones provided by \chameleon{}. Using heuristic methods may achieve the best of both worlds. It may produce effective results by guessing the possible un-typable expression based on the position it appears in the syntax tree and project structure or guessing the intended type signature by comparing the number of supporting slices in the source code.

\subsection{Threats to validity}

\todo{Put threats BEFORE Discussion and at end Eval section?}

\todo{These are too terse/unclear - need expanding. Suggest classify as internal / external threats etc .  What about background and number of participants as threats??}

One consequence of our recruiting approach is it is harder for previous participants to enter a later study. To minimize the lurking variable of previous experience, we use new code challenges in every study and conduct trial runs in every study to bring new participants up to speed.

Conducting studies remotely and unsupervised is that intervening when users encounter usability issues mid-study is impossible. To ensure there is no major usability issue that could lower the quality of data collection, we conducted cognitive walkthroughs and sandbox pilots before running each study.